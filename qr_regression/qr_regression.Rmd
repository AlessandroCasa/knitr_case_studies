---
title: "The QR Decomposition For Regression Models"
author: "Michael Betancourt"
date: "July 2017"
output:
  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA)
```

A common problem in regression modeling is correlation amongst the covariates
which can induce strong posterior correlations that frustrate accurate
computation.  In this case study I will review the  _QR decomposition_, a
technique for decorrelating covariates and, consequently, the resulting
posterior distribution.

We'll begin with a simple example that demonstrates the difficulties induced
by correlated covariates before going through the mathematics of the QR
decomposition and finally how it can be applied in Stan.

# Setting up the RStan Environment

First things first, let's setup our local environment,

```{r, comment=NA}
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
source("stan_utility.R")

c_light <- c("#DCBCBC")
c_light_highlight <- c("#C79999")
c_mid <- c("#B97C7C")
c_mid_highlight <- c("#A25050")
c_dark <- c("#8F2727")
c_dark_highlight <- c("#7C0000")

c_light_trans <- c("#DCBCBC80")
c_light_highlight_trans <- c("#C7999980")
c_mid_trans <- c("#B97C7C80")
c_mid_highlight_trans <- c("#A2505080")
c_dark_trans <- c("#8F272780")
c_dark_highlight_trans <- c("#7C000080")
```

# Fitting Issues with Correlated Covariates

Now consider a very simple regression with only two covariates --
$x \sim \mathcal{N} (1, 0.1)$ and it's square, $x^{2}$. The inclusion of both
$x$ and $x^{2}$ is not uncommon in polynomial regressions where the response is
given by a sum of polynomials over the input covariates.

In particular, the correlations here are particularly strong because we didn't
standardize the covariate, $x$, before squaring it.  Powers are much better
identified when the input is centered around zero.  Unfortunately in practice
we may not be able to standardize the covariates before they are transformed.
Moreover, in more complex regressions seemingly independent covariates are
often highly correlated due to common confounders in which case standardization
will not be of much help.

We begin by simulating some data and, per good Stan practice, saving it in an
external file,

```{r, comment=NA}
set.seed(689934)

N <- 5000
x <- rnorm(N, 10, 1)
X = t(data.matrix(data.frame(x, x * x)))

M <- 2
beta = matrix(c(2.5, -1), nrow=M, ncol=1)
alpha <- -0.275
sigma <- 0.8

mu <- t(X) %*% beta + alpha
y = sapply(1:N, function(n) rnorm(1, mu[n], sigma))

stan_rdump(c("N", "M", "X", "y"), file="regr.data.R")
```

Because the covariate $x$ is restricted to positive values, it is highly
correlated with its square,

```{r}
par(mar = c(4, 4, 0.5, 0.5))
plot(X[1,], X[2,],
     col=c_dark, pch=16, cex=0.8, xlab="x", ylab="x^2")
```

With the data in hand we can attempt to fit a naive linear regression model,

```{r, comment=NA}
writeLines(readLines("regr.stan"))
```

```{r, cache=TRUE, comment=NA}
input_data <- read_rdump("regr.data.R")
fit <- stan(file='regr.stan', data=input_data, seed=483892929)
```

Checking our diagnostics,

```{r, comment=NA}
check_all_diagnostics(fit)
```

we see that everything looks okay save for some trajectories that have
prematurely terminated because of the default tree depth limit.  Although there
aren't many of them,

```{r, comment=NA}
breaks <- 0:10
sampler_params <- get_sampler_params(fit, inc_warmup=FALSE)
treedepths <- do.call(rbind, sampler_params)[,'treedepth__']
treedepths_hist <- hist(treedepths, breaks=breaks, plot=FALSE)

par(mar = c(4, 4, 0.5, 0.5))
plot(treedepths_hist, col=c_dark_highlight_trans, main="",
     xlab="theta.1", yaxt='n', ann=FALSE)
```

even a few prematurely terminating trajectories can hinder performance, and
may even indicate potential problems with adaptation.

To maximize performance and avoid any potential issues we refit with a larger
tree depth threshold,

```
fit <- stan(file='regr.stan', data=input_data, seed=483892929, control=list(max_treedepth=15))
```

Now the tree depth diagnostics are clean

```{r, comment=NA}
check_treedepth(fit, 15)
```

The small step sizes, however,

```{r, comment=NA}
sampler_params <- get_sampler_params(fit, inc_warmup=FALSE)
stepsizes <- sapply(sampler_params, function(x) x[1,'stepsize__'])
names(stepsizes) <- list("Chain 1", "Chain 2", "Chain 3" ,"Chain 4")
stepsizes
```

do require significant computation, over one million gradient evaluations,
across the entire fit,

```{r, comment=NA}
n_gradients <- sapply(sampler_params, function(x) sum(x[,'n_leapfrog__']))
n_gradients
sum(n_gradients)
```

Plotting the posterior samples we can see why.  The marginal posterior for the
slopes is only weakly-identified and the posterior geometry becomes extremely
narrow,

```{r}
partition <- partition_div(fit)
params <- partition[[2]]

par(mar = c(4, 4, 0.5, 0.5))
plot(params$'beta[1]', params$'beta[2]',
     col=c_dark_trans, pch=16, cex=0.8, xlab="beta[1]", ylab="beta[2]",
     xlim=c(1.5, 3), ylim=c(-1.1, -0.9))
points(beta[1,1], beta[2,1],
       col=c_mid, pch=17, cex=2)
```

requiring very precise trajectory simulations at each iteration of the Markov
chains.

# Decorrelating the Posterior with a QR Decomposition

Fortunately we can reduce the correlations between the covariates, and
ameliorate the challenging geometry of the Bayesian posterior, by applying a QR
decomposition.  Perhaps unsurprisingly this is the same QR decomposition that
arises in the analytic maximum likelihood and conjugate Bayesian treatment of
linear regression, although here it will be applicable regardless of the choice
of priors and for any general linear model.

## Mathematical Derivation

The _thin_ QR decomposition decomposes a rectangular $N \times M$ matrix
into
$$
\mathbf{A} = \mathbf{Q} \cdot \mathbf{R}
$$
where $\mathbf{Q}$ is an $N \times M$ orthogonal matrix with $M$ non-zero rows
and $N - M$ rows of vanishing rows, and $\mathbf{R}$ is a $M \times M$
upper-triangular matrix.

If we apply the decomposition to the transposed design matrix,
$\mathbf{X}^{T} = \mathbf{Q} \cdot \mathbf{R}$, then we can refactor the linear
response as
$$
\begin{align*}
\boldsymbol{\mu}
&= \mathbf{X}^{T} \cdot \boldsymbol{\beta} + \alpha
\\
&= \mathbf{Q} \cdot \mathbf{R} \cdot \boldsymbol{\beta} + \alpha
\\
&= \mathbf{Q} \cdot (\mathbf{R} \cdot \boldsymbol{\beta}) + \alpha
\\
&= \mathbf{Q} \cdot \widetilde{\boldsymbol{\beta}} + \alpha.
\\
\end{align*}
$$

Because the matrix $\mathbf{Q}$ is orthogonal, its columns are independent and
consequently we expect the posterior over the new parameters,
$\widetilde{\boldsymbol{\beta}} = \mathbf{R} \cdot \boldsymbol{\beta}$,
to be significantly less correlated.  In practice we can also equalize the
scales of the posterior by normalizing the $Q$ and $R$ matrices,
$$
\begin{align*}
\mathbf{Q} &\rightarrow \mathbf{Q} \cdot N
\\
\mathbf{R} &\rightarrow \mathbf{R} \, / \, N.
\end{align*}
$$

We can then readily recover the original slopes as
$$
\boldsymbol{\beta} = \mathbf{R}^{-1} \cdot \widetilde{\boldsymbol{\beta}}.
$$
As $\mathbf{R}$ is upper diagonal we could compute its inverse with only
$\mathcal{O} (M^{2})$ operations, but because we need to compute it only once
we will use the naive inversion function in Stan here.

Because the transformation between $\boldsymbol{\beta}$ and
$\widetilde{\boldsymbol{\beta}}$ is _linear_, the corresponding Jacobian depends
only on the data and hence doesn't affect posterior computations.  This means
that in Stan we can define the transformed parameters
$\boldsymbol{\beta} = \mathbf{R}^{-1} \cdot \widetilde{\boldsymbol{\beta}}$
and apply priors directly to $\boldsymbol{\beta}$ while ignoring the warning
about Jacobians.

Interestingly, applying weakly-informative priors to the
$\widetilde{\boldsymbol{\beta}}$ directly can be interpreted as a form
of _empirical Bayes_, where we use the empirical correlations in the data to
guide the choice of prior.

## Implementation in Stan

The scaled, thin QR decomposition is straightforward to implement in Stan,

```{r, comment=NA}
writeLines(readLines("qr_regr.stan"))
```

Fitting the QR regression model, and ignoring the warning about the Jacobian
due to the considerations above,

```{r, cache=TRUE, comment=NA}
qr_fit <- stan(file='qr_regr.stan', data=input_data, seed=483892929)
```

we see no indications of an inaccurate fit,

```{r, comment=NA}
check_all_diagnostics(qr_fit)
```

The effective sample sizes are the same, but the larger step sizes,

```{r, comment=NA}
sampler_params <- get_sampler_params(qr_fit, inc_warmup=FALSE)
qr_stepsizes <- sapply(sampler_params, function(x) x[1,'stepsize__'])
names(qr_stepsizes) <- list("Chain 1", "Chain 2", "Chain 3" ,"Chain 4")
qr_stepsizes
```

require only about half the gradient evaluations needed in the naive regression,

```{r, comment=NA}
n_gradients <- sapply(sampler_params, function(x) sum(x[,'n_leapfrog__']))
n_gradients
sum(n_gradients)
```

Consequently even in this simple example the QR decomposition is about twice
as fast as the naive regression.  In more complex, higher-dimensional
regressions the improvement can be even larger.

This is not unexpected, however, given how much less correlated the posterior
for the transformed slopes is,

```{r}
partition <- partition_div(qr_fit)
params <- partition[[2]]

par(mar = c(4, 4, 0.5, 0.5))
plot(params$'beta_tilde[1]', params$'beta_tilde[2]',
     col=c_dark_trans, pch=16, cex=0.8, xlab="beta_tilde[1]", ylab="beta_tilde[2]")
```

Comfortingly, we also successfully recover the posterior for the nominal slopes,

```{r}
par(mar = c(4, 4, 0.5, 0.5))
plot(params$'beta[1]', params$'beta[2]',
     col=c_dark_trans, pch=16, cex=0.8, xlab="beta[1]", ylab="beta[2]",
     xlim=c(1.5, 3), ylim=c(-1.1, -0.9))
points(beta[1,1], beta[2,1],
       col=c_mid, pch=17, cex=2)
```

## The Importance of Centering Covariates

If the rows of the effective design matrix, $\mathbf{Q}$, are orthogonal, then
why are the transformed slopes nontrivially correlated in the QR regression
posterior?

One possibility could be the prior we put on the nominal slopes, which implies a
strongly correlated prior for the transformed slopes.  Here, however, the prior
is too weak to have any strong effect on the posterior distribution.  Still,
it's important to keep in mind that the QR decomposition performs best when the
likelihood dominates the prior, either due to sufficiently many data or
sufficiently weak prior information.

The real cause of the correlations in the posterior for the transformed slopes
is that _the covariates are not centered_.  As with any decomposition, the QR
decomposition can fully decorrelate the covariates, and hence the likelihood
and the corresponding posterior, only after the covariates have been centered
around their empirical means.

Our design matrix is readily recentered within Stan itself, although we just as
easily could have done it within R itself.  Keeping in mind that centering the
covariates drastically changes the interpretation of the intercept, we also
should inflate the prior for $\alpha$,

```{r, comment=NA}
writeLines(readLines("qr_regr_centered.stan"))
```

and then fit the recentered design matrix,

```{r, cache=TRUE, warning=TRUE, message=TRUE, comment=NA}
qr_fit <- stan(file='qr_regr_centered.stan', data=input_data, seed=483892929)
```

```{r, comment=NA}
check_all_diagnostics(qr_fit)
```

Not only has the effective sample size has drastically increased, the fit
requires only a tenth of the gradient evaluations needed by the naive
regression,

```{r, comment=NA}
sampler_params <- get_sampler_params(qr_fit, inc_warmup=FALSE)
n_gradients <- sapply(sampler_params, function(x) sum(x[,'n_leapfrog__']))
n_gradients
sum(n_gradients)
```

With the improved effective sample size and reduced computational cost, the
centered QR decomposition achieves a 20 fold increase in performance!

All of this is due to the now isotropic posterior for the transformed slopes,

```{r}
partition <- partition_div(qr_fit)
params <- partition[[2]]

par(mar = c(4, 4, 0.5, 0.5))
plot(params$'beta_tilde[1]', params$'beta_tilde[2]',
     col=c_dark_trans, pch=16, cex=0.8, xlab="beta_tilde[1]", ylab="beta_tilde[2]")
```

While the posterior for the new intercept is expectedly different, the posterior
for the nominal slopes remains the same,

```{r}
par(mar = c(4, 4, 0.5, 0.5))
plot(params$'beta[1]', params$'beta[2]',
     col=c_dark_trans, pch=16, cex=0.8, xlab="beta[1]", ylab="beta[2]",
     xlim=c(1.5, 3), ylim=c(-1.1, -0.9))
points(beta[1,1], beta[2,1],
       col=c_mid, pch=17, cex=2)
```

A common feature of regression models, centering not only improves the
interpretability of the model but also proves critical to achieving optimal
computational performance.

# Conclusion

The QR decomposition is a straightforward technique that can drastically improve
the performance of regression with not only linear models but also general
linear models.  Given its ease of use and strong potential for improvement it
should be a ready tool in any modeler's toolbox.

# Acknowledgements

The exact implementation used here was cribbed from the discussion of QR
decomposition in the Stan manual written by Ben Goodrich, who also originally
introduced the technique into the Stan ecosystem.

# Original Computing Environment

```{r, comment=NA}
writeLines(readLines(file.path(Sys.getenv("HOME"), ".R/Makevars")))
```

```{r, comment=NA}
devtools::session_info("rstan")
```
